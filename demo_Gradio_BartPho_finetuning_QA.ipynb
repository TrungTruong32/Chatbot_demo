{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community\n",
        "!pip install gradio transformers torch\n",
        "!pip install qdrant-client\n"
      ],
      "metadata": {
        "id": "3nzxYIBZQDjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Qdrant\n",
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from qdrant_client import QdrantClient\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "bi4aB92AQQcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cài đặt thông tin truy cập vector database\n",
        "QDRANT_API_KEY = \"...\" #Tạo ở Qdrant\n",
        "QDRANT_URL = \"...\"\n",
        "HUGGINGFACE_API_KEY= \"...\""
      ],
      "metadata": {
        "id": "AQ2tuKisQdbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQAvuj3nP9qZ"
      },
      "outputs": [],
      "source": [
        "# Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Hệ thống hỏi đáp về các văn bản:\")\n",
        "    from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "    import torch\n",
        "    import gradio as gr\n",
        "    from qdrant_client import QdrantClient\n",
        "    import numpy as np\n",
        "\n",
        "    # Khởi tạo mô hình và tokenizer cho QA\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"PhucDanh/Bartpho-fine-tuning-model-for-question-answering\")\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(\"PhucDanh/Bartpho-fine-tuning-model-for-question-answering\")\n",
        "    # Delete colection if exist to reup all documents\n",
        "    client = QdrantClient(\n",
        "        url=QDRANT_URL, prefer_grpc=False, api_key=QDRANT_API_KEY,\n",
        "    )\n",
        "\n",
        "    # Load the embedding model\n",
        "    embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
        "        model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
        "        api_key = HUGGINGFACE_API_KEY,\n",
        "        #model_kwargs = {'device': 'auto'}\n",
        "    )\n",
        "    collection_name = \"Data_chatbot\"\n",
        "\n",
        "    def search_from_qdrant(query, top_k=2):\n",
        "        try:\n",
        "            # Tạo đối tượng db kết nối với Qdrant và sử dụng embeddings đã được khởi tạo\n",
        "            db = Qdrant(client=client, embeddings=embeddings, collection_name=collection_name)\n",
        "\n",
        "            # Tìm kiếm tương đồng với điểm số (kết quả trả về là một danh sách các tài liệu)\n",
        "            docs = db.similarity_search_with_score(query=query, k=2)\n",
        "\n",
        "            # for doc, score in docs:  # Unpack the tuple into doc and score\n",
        "                # print(doc.page_content)  # Access page_content directly from the Document object\n",
        "                # print(doc.metadata)\n",
        "\n",
        "            # Lấy nội dung của các tài liệu từ payload\n",
        "            contexts = [doc.page_content for doc, score in docs]  # Lấy 'page_content'\n",
        "            return contexts\n",
        "        except Exception as e:\n",
        "            print(\"Lỗi khi tìm kiếm từ Qdrant:\", e)\n",
        "            return []\n",
        "\n",
        "\n",
        "    # contexts = search_from_qdrant(question)\n",
        "\n",
        "\n",
        "    # Hàm trả lời câu hỏi với context tìm được từ Qdrant\n",
        "    def answer_question(question, contexts):\n",
        "        # Tìm context từ Qdrant\n",
        "        # contexts = search_from_qdrant(question)\n",
        "        # context = \" \".join(contexts)  # Kết hợp các context lại thành một đoạn văn bản dài\n",
        "\n",
        "        # Tiến hành trả lời câu hỏi\n",
        "        inputs = tokenizer(question, contexts, return_tensors=\"pt\")\n",
        "        if 'token_type_ids' in inputs:\n",
        "            del inputs['token_type_ids']\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        answer_start_index = outputs.start_logits.argmax()\n",
        "        answer_end_index = outputs.end_logits.argmax()\n",
        "\n",
        "        predict_answer_tokens = inputs.input_ids[0, answer_start_index: answer_end_index + 1]\n",
        "        return tokenizer.decode(predict_answer_tokens)\n",
        "    def process_question(question):\n",
        "        # Tìm kiếm context từ Qdrant\n",
        "        contexts = search_from_qdrant(question)\n",
        "        context = \" \".join(contexts)\n",
        "        if not context:\n",
        "            return \"Không tìm thấy thông tin phù hợp trong dữ liệu.\"\n",
        "\n",
        "        # Trả lời câu hỏi dựa trên context\n",
        "        answer = answer_question(question, context)\n",
        "        return answer\n",
        "\n",
        "\n",
        "\n",
        "    # gr.Markdown(\"# Hệ thống hỏi đáp về các văn bản:\")\n",
        "\n",
        "    with gr.Row():\n",
        "        txt_question = gr.Textbox(lines=2, label=\"Câu hỏi\")\n",
        "        btn_answer = gr.Button(\"Tìm câu trả lời\")\n",
        "\n",
        "    with gr.Row():\n",
        "        txt_answer = gr.Textbox(label=\"Câu trả lời:\", lines=5)\n",
        "\n",
        "    # Kết nối chức năng trả lời câu hỏi với nút bấm\n",
        "    btn_answer.click(fn=process_question, inputs=txt_question, outputs=txt_answer)\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    }
  ]
}